mistranslation: 0.911
architecture: 0.901
user-level: 0.892
device: 0.848
semantic: 0.806
graphic: 0.765
debug: 0.654
performance: 0.632
register: 0.617
risc-v: 0.569
i386: 0.555
x86: 0.552
peripherals: 0.515
kernel: 0.503
arm: 0.487
socket: 0.447
PID: 0.422
boot: 0.420
vnc: 0.417
virtual: 0.404
network: 0.381
ppc: 0.357
hypervisor: 0.342
VMM: 0.304
permissions: 0.300
files: 0.264
TCG: 0.255
assembly: 0.244
KVM: 0.142
--------------------
debug: 0.986
register: 0.921
x86: 0.916
kernel: 0.840
assembly: 0.827
virtual: 0.183
hypervisor: 0.108
TCG: 0.041
i386: 0.040
arm: 0.015
user-level: 0.008
PID: 0.005
risc-v: 0.005
performance: 0.005
semantic: 0.004
architecture: 0.004
device: 0.004
files: 0.004
ppc: 0.004
KVM: 0.002
boot: 0.002
network: 0.002
VMM: 0.001
graphic: 0.001
socket: 0.001
permissions: 0.001
peripherals: 0.000
vnc: 0.000
mistranslation: 0.000

sysret sets invalid ss

I'm developing an OS. I use only sysret to enter user space. When an interrupt occurred, it would GPF on iretq'ing from it. On investigating, the cs on the stack is 0x2b (valid and correct). The ss on the stack is 0x20, which has a rpl of 0 which is incorrect. iretq checks that and gpf's. Making the irq handler manually modify it to 0x23 fixes it locally.

This happens on the non-kvm'ed qemu. I haven't tried the kvm'ed one. Qemu version 0.12.5. I haven't tried with the current development version either.

Minor update, I found that I made a mistake with the value in STAR. I loaded 0x18 into the top 16 bits, so the bug is not that SS is 0x20 (which I thought to be wrong but was my own mistake). Why does it set CS=0x2b from that value & run?

