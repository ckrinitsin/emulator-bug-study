risc-v: 0.861
user-level: 0.794
mistranslation: 0.785
peripherals: 0.742
device: 0.733
permissions: 0.732
KVM: 0.726
register: 0.722
debug: 0.716
kernel: 0.713
performance: 0.712
socket: 0.712
boot: 0.711
vnc: 0.708
network: 0.707
arm: 0.707
architecture: 0.706
assembly: 0.701
virtual: 0.696
VMM: 0.696
PID: 0.693
ppc: 0.682
files: 0.679
semantic: 0.677
graphic: 0.673
x86: 0.666
TCG: 0.663
hypervisor: 0.622
i386: 0.604

disk missing in the guest contingently when hotplug several virtio scsi disks consecutively

Hi, I found a bug that disk missing (not all disks missing ) in the guest contingently when hotplug several virtio scsi disks consecutively.  After rebooting the guest,the missing disks appear again.

The guest is centos7.3 running on a centos7.3 host and the scsi controllers are configed with iothread.  The scsi controller xml is below:

    <controller type='scsi' index='0' model='virtio-scsi'>
      <driver iothread='26'/>
      <alias name='scsi0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </controller>


If the scsi controllers are configed without iothread,  disks are all can be seen in the guest when hotplug several virtio scsi disks consecutively.

I think the biggest difference between them is that scsi controllers with iothread call virtio_notify_irqfd to notify guest and scsi controllers without

iothread call virtio_notify instead.  What make it difference? Will  interrupts are lost when call virtio_notify_irqfd  due to  race condition for some unknow reasons? Maybe guys more familiar with scsi dataplane can help. Thanks for your reply!

Please post the following information:
(host)# rpm -qa | grep qemu-kvm
(guest)# uname -r

What are the exact steps to reproduce this issue (virsh command-lines and XML)?

I also met this bug

Hi, Stefan.
(host)# rpm -qa | grep qemu-kvm
qemu-kvm-2.8.1-25.142.x86_64
(guest)# uname -r
3.10.0-514.el7.x86_64

I also tried the newest version of qemu-kvm, but it also met this issue.
The steps to reproduce this issue are below:

1)attach four virtio-scsi controller with dataplane to vm.
     <controller type='scsi' index='0' model='virtio-scsi'>
       <driver iothread='1'/>
       <alias name='scsi0'/>
       <address type='pci' domain='0x0000' bus='0x00' slot='0x07' 
function='0x0'/>
     </controller>
     <controller type='scsi' index='1' model='virtio-scsi'>
       <driver iothread='2'/>
       <alias name='scsi1'/>
       <address type='pci' domain='0x0000' bus='0x00' slot='0x0a' 
function='0x0'/>
     </controller>
     <controller type='scsi' index='2' model='virtio-scsi'>
       <driver iothread='3'/>
       <alias name='scsi2'/>
       <address type='pci' domain='0x0000' bus='0x00' slot='0x0b' 
function='0x0'/>
     </controller>
     <controller type='scsi' index='3' model='virtio-scsi'>
       <driver iothread='4'/>
       <alias name='scsi3'/>
       <address type='pci' domain='0x0000' bus='0x00' slot='0x0c' 
function='0x0'/>
     </controller>

2)attach 35 virtio-scsi disks(sda - sdai) to vm consecutively. One 
controller has 15 scsi disks.
A example of disk xml is below:
     <disk type='block' device='lun' rawio='yes'>
       <driver name='qemu' type='raw' cache='none' io='native'/>
       <source dev='/dev/mapper/360022a11000c1e0a0787bb2500000139'/>
       <backingStore/>
       <target dev='sda' bus='scsi'/>
       <shareable/>
       <alias name='scsi0-0-0-0'/>
       <address type='drive' controller='0' bus='0' target='0' unit='0'/>
     </disk>

    You can write a shell script like this:
        for((i=1;i++;i<=35))
        do
             virsh attach-device centos7.3_64_server scsi_disk_$i.xml 
--config --live
        done

This issue is a probabilistic event. If it does not appear, repeat the 
above steps several more times.
Thank you!

On 2018/6/28 21:01, Stefan Hajnoczi wrote:
> Please post the following information:
> (host)# rpm -qa | grep qemu-kvm
> (guest)# uname -r
>
> What are the exact steps to reproduce this issue (virsh command-lines
> and XML)?
>



The QEMU project is currently considering to move its bug tracking to another system. For this we need to know which bugs are still valid and which could be closed already. Thus we are setting older bugs to "Incomplete" now.
If you still think this bug report here is valid, then please switch the state back to "New" within the next 60 days, otherwise this report will be marked as "Expired". Or mark it as "Fix Released" if the problem has been solved with a newer version of QEMU already. Thank you and sorry for the inconvenience.

[Expired for QEMU because there has been no activity for 60 days.]

